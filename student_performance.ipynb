# Import necessary libraries
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler
from sklearn.ensemble import RandomForestRegressor
from sklearn.metrics import mean_squared_error, r2_score

# Load the dataset
data = pd.read_csv('student-mat.csv', sep=';')

# Selecting relevant features for attendance, study habits, and socio-economic factors
selected_features = [
    'absences',         # Attendance
    'studytime',        # Study habits
    'failures',         # Study habits
    'schoolsup',        # Study habits - extra support
    'famsup',           # Family support (study habit related)
    'famsize',          # Socio-economic factor
    'Pstatus',          # Socio-economic factor
    'Medu',             # Mother's education
    'Fedu',             # Father's education
    'higher',           # Wants higher education
    'internet',         # Access to the internet
    'G3'                # Target variable: Final grade
]

# Filter dataset based on the selected features
data_filtered = data[selected_features]

# Convert categorical variables to numeric using One-Hot Encoding
data_encoded = pd.get_dummies(data_filtered, drop_first=True)

# Define features (X) and target (y)
X = data_encoded.drop('G3', axis=1)  # Features
y = data_encoded['G3']               # Target (final grades)

# Split the dataset into training and testing sets
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

# Feature scaling (standardizing the dataset)
scaler = StandardScaler()
X_train = scaler.fit_transform(X_train)
X_test = scaler.transform(X_test)

# Model selection: Random Forest Regressor
model = RandomForestRegressor(n_estimators=100, random_state=42)
model.fit(X_train, y_train)  # Train the model

# Make predictions on the test set
y_pred = model.predict(X_test)

# Model evaluation
mse = mean_squared_error(y_test, y_pred)  # Mean Squared Error
r2 = r2_score(y_test, y_pred)  # R-squared score (explained variance)

# Print evaluation results
print(f'Mean Squared Error: {mse}')
print(f'R-squared: {r2}')

# Visualize the Actual vs Predicted Grades
plt.figure(figsize=(10, 6))
plt.scatter(y_test, y_pred, color='blue')
plt.plot([min(y_test), max(y_test)], [min(y_test), max(y_test)], color='red', lw=2)
plt.xlabel('Actual Grades')
plt.ylabel('Predicted Grades')
plt.title('Actual vs Predicted Grades')
plt.show()

# Visualize the effect of features on grades
# 1. Boxplot for 'studytime'
plt.figure(figsize=(10, 6))
sns.boxplot(x='studytime', y='G3', data=data_filtered)
plt.title('Effect of Study Time on Final Grades')
plt.xlabel('Study Time')
plt.ylabel('Final Grades')
plt.show()

# 2. Boxplot for 'famsup'
plt.figure(figsize=(10, 6))
sns.boxplot(x='famsup', y='G3', data=data_filtered)
plt.title('Effect of Family Support on Final Grades')
plt.xlabel('Family Support')
plt.ylabel('Final Grades')
plt.show()

# 3. Correlation heatmap
plt.figure(figsize=(12, 8))
correlation_matrix = data_encoded.corr()
sns.heatmap(correlation_matrix, annot=True, fmt=".2f", cmap='coolwarm', square=True)
plt.title('Correlation Heatmap of Features and Final Grades')
plt.show()
